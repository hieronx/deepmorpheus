{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.1562933921813965.\n",
      "Epoch 2: Loss = 1.136213779449463.\n",
      "Epoch 3: Loss = 1.1181296110153198.\n",
      "Epoch 4: Loss = 1.101839542388916.\n",
      "Epoch 5: Loss = 1.087153434753418.\n",
      "Epoch 6: Loss = 1.073894739151001.\n",
      "Epoch 7: Loss = 1.0618999004364014.\n",
      "Epoch 8: Loss = 1.0510190725326538.\n",
      "Epoch 9: Loss = 1.0411165952682495.\n",
      "Epoch 10: Loss = 1.0320698022842407.\n",
      "Epoch 11: Loss = 1.0237685441970825.\n",
      "Epoch 12: Loss = 1.0161144733428955.\n",
      "Epoch 13: Loss = 1.0090205669403076.\n",
      "Epoch 14: Loss = 1.0024094581604004.\n",
      "Epoch 15: Loss = 0.9962127804756165.\n",
      "Epoch 16: Loss = 0.9903705716133118.\n",
      "Epoch 17: Loss = 0.984829843044281.\n",
      "Epoch 18: Loss = 0.9795444011688232.\n",
      "Epoch 19: Loss = 0.9744738340377808.\n",
      "Epoch 20: Loss = 0.9695825576782227.\n",
      "Epoch 21: Loss = 0.9648394584655762.\n",
      "Epoch 22: Loss = 0.9602174162864685.\n",
      "Epoch 23: Loss = 0.9556930065155029.\n",
      "Epoch 24: Loss = 0.9512450695037842.\n",
      "Epoch 25: Loss = 0.9468557834625244.\n",
      "Epoch 26: Loss = 0.9425090551376343.\n",
      "Epoch 27: Loss = 0.938191294670105.\n",
      "Epoch 28: Loss = 0.9338899850845337.\n",
      "Epoch 29: Loss = 0.9295947551727295.\n",
      "Epoch 30: Loss = 0.9252963662147522.\n",
      "Epoch 31: Loss = 0.9209864139556885.\n",
      "Epoch 32: Loss = 0.9166578650474548.\n",
      "Epoch 33: Loss = 0.9123044013977051.\n",
      "Epoch 34: Loss = 0.9079204201698303.\n",
      "Epoch 35: Loss = 0.9035011529922485.\n",
      "Epoch 36: Loss = 0.8990424871444702.\n",
      "Epoch 37: Loss = 0.894540548324585.\n",
      "Epoch 38: Loss = 0.8899924159049988.\n",
      "Epoch 39: Loss = 0.8853950500488281.\n",
      "Epoch 40: Loss = 0.8807461261749268.\n",
      "Epoch 41: Loss = 0.8760437965393066.\n",
      "Epoch 42: Loss = 0.8712860345840454.\n",
      "Epoch 43: Loss = 0.8664717674255371.\n",
      "Epoch 44: Loss = 0.8615995645523071.\n",
      "Epoch 45: Loss = 0.8566685914993286.\n",
      "Epoch 46: Loss = 0.851678192615509.\n",
      "Epoch 47: Loss = 0.8466278910636902.\n",
      "Epoch 48: Loss = 0.841517448425293.\n",
      "Epoch 49: Loss = 0.8363466262817383.\n",
      "Epoch 50: Loss = 0.8311156630516052.\n",
      "Epoch 51: Loss = 0.8258247971534729.\n",
      "Epoch 52: Loss = 0.82047438621521.\n",
      "Epoch 53: Loss = 0.8150649666786194.\n",
      "Epoch 54: Loss = 0.8095972537994385.\n",
      "Epoch 55: Loss = 0.8040720820426941.\n",
      "Epoch 56: Loss = 0.7984904050827026.\n",
      "Epoch 57: Loss = 0.7928532361984253.\n",
      "Epoch 58: Loss = 0.7871618270874023.\n",
      "Epoch 59: Loss = 0.7814173698425293.\n",
      "Epoch 60: Loss = 0.7756214141845703.\n",
      "Epoch 61: Loss = 0.7697753310203552.\n",
      "Epoch 62: Loss = 0.7638805508613586.\n",
      "Epoch 63: Loss = 0.7579388618469238.\n",
      "Epoch 64: Loss = 0.7519518136978149.\n",
      "Epoch 65: Loss = 0.7459214925765991.\n",
      "Epoch 66: Loss = 0.739849328994751.\n",
      "Epoch 67: Loss = 0.7337374687194824.\n",
      "Epoch 68: Loss = 0.7275879383087158.\n",
      "Epoch 69: Loss = 0.7214025855064392.\n",
      "Epoch 70: Loss = 0.7151836156845093.\n",
      "Epoch 71: Loss = 0.7089328765869141.\n",
      "Epoch 72: Loss = 0.7026529908180237.\n",
      "Epoch 73: Loss = 0.6963457465171814.\n",
      "Epoch 74: Loss = 0.6900137066841125.\n",
      "Epoch 75: Loss = 0.683659017086029.\n",
      "Epoch 76: Loss = 0.6772841215133667.\n",
      "Epoch 77: Loss = 0.6708914041519165.\n",
      "Epoch 78: Loss = 0.6644832491874695.\n",
      "Epoch 79: Loss = 0.658062219619751.\n",
      "Epoch 80: Loss = 0.651630699634552.\n",
      "Epoch 81: Loss = 0.6451912522315979.\n",
      "Epoch 82: Loss = 0.6387465000152588.\n",
      "Epoch 83: Loss = 0.6322987675666809.\n",
      "Epoch 84: Loss = 0.6258506178855896.\n",
      "Epoch 85: Loss = 0.6194046139717102.\n",
      "Epoch 86: Loss = 0.612963080406189.\n",
      "Epoch 87: Loss = 0.6065284609794617.\n",
      "Epoch 88: Loss = 0.6001029014587402.\n",
      "Epoch 89: Loss = 0.5936888456344604.\n",
      "Epoch 90: Loss = 0.5872880220413208.\n",
      "Epoch 91: Loss = 0.580902636051178.\n",
      "Epoch 92: Loss = 0.57453453540802.\n",
      "Epoch 93: Loss = 0.5681852102279663.\n",
      "Epoch 94: Loss = 0.5618562698364258.\n",
      "Epoch 95: Loss = 0.5555490255355835.\n",
      "Epoch 96: Loss = 0.549264669418335.\n",
      "Epoch 97: Loss = 0.5430041551589966.\n",
      "Epoch 98: Loss = 0.53676837682724.\n",
      "Epoch 99: Loss = 0.5305579900741577.\n",
      "Epoch 100: Loss = 0.524373471736908.\n",
      "Epoch 101: Loss = 0.5182151198387146.\n",
      "Epoch 102: Loss = 0.5120832920074463.\n",
      "Epoch 103: Loss = 0.505977988243103.\n",
      "Epoch 104: Loss = 0.49989908933639526.\n",
      "Epoch 105: Loss = 0.4938466250896454.\n",
      "Epoch 106: Loss = 0.48782026767730713.\n",
      "Epoch 107: Loss = 0.48181986808776855.\n",
      "Epoch 108: Loss = 0.47584500908851624.\n",
      "Epoch 109: Loss = 0.4698954224586487.\n",
      "Epoch 110: Loss = 0.46397075057029724.\n",
      "Epoch 111: Loss = 0.45807069540023804.\n",
      "Epoch 112: Loss = 0.4521949291229248.\n",
      "Epoch 113: Loss = 0.4463433027267456.\n",
      "Epoch 114: Loss = 0.44051554799079895.\n",
      "Epoch 115: Loss = 0.4347115755081177.\n",
      "Epoch 116: Loss = 0.42893147468566895.\n",
      "Epoch 117: Loss = 0.42317527532577515.\n",
      "Epoch 118: Loss = 0.41744327545166016.\n",
      "Epoch 119: Loss = 0.41173574328422546.\n",
      "Epoch 120: Loss = 0.40605324506759644.\n",
      "Epoch 121: Loss = 0.40039631724357605.\n",
      "Epoch 122: Loss = 0.394765704870224.\n",
      "Epoch 123: Loss = 0.3891623020172119.\n",
      "Epoch 124: Loss = 0.3835870623588562.\n",
      "Epoch 125: Loss = 0.37804117798805237.\n",
      "Epoch 126: Loss = 0.3725258708000183.\n",
      "Epoch 127: Loss = 0.3670424222946167.\n",
      "Epoch 128: Loss = 0.3615923821926117.\n",
      "Epoch 129: Loss = 0.35617730021476746.\n",
      "Epoch 130: Loss = 0.35079875588417053.\n",
      "Epoch 131: Loss = 0.345458447933197.\n",
      "Epoch 132: Loss = 0.3401581645011902.\n",
      "Epoch 133: Loss = 0.3348996639251709.\n",
      "Epoch 134: Loss = 0.32968470454216003.\n",
      "Epoch 135: Loss = 0.32451507449150085.\n",
      "Epoch 136: Loss = 0.31939274072647095.\n",
      "Epoch 137: Loss = 0.31431934237480164.\n",
      "Epoch 138: Loss = 0.3092966675758362.\n",
      "Epoch 139: Loss = 0.30432644486427307.\n",
      "Epoch 140: Loss = 0.29941025376319885.\n",
      "Epoch 141: Loss = 0.294549822807312.\n",
      "Epoch 142: Loss = 0.2897464334964752.\n",
      "Epoch 143: Loss = 0.285001665353775.\n",
      "Epoch 144: Loss = 0.2803167402744293.\n",
      "Epoch 145: Loss = 0.275692880153656.\n",
      "Epoch 146: Loss = 0.27113115787506104.\n",
      "Epoch 147: Loss = 0.266632616519928.\n",
      "Epoch 148: Loss = 0.2621981203556061.\n",
      "Epoch 149: Loss = 0.25782832503318787.\n",
      "Epoch 150: Loss = 0.2535238564014435.\n",
      "Epoch 151: Loss = 0.24928529560565948.\n",
      "Epoch 152: Loss = 0.24511301517486572.\n",
      "Epoch 153: Loss = 0.24100731313228607.\n",
      "Epoch 154: Loss = 0.23696833848953247.\n",
      "Epoch 155: Loss = 0.2329961210489273.\n",
      "Epoch 156: Loss = 0.2290905863046646.\n",
      "Epoch 157: Loss = 0.225251704454422.\n",
      "Epoch 158: Loss = 0.22147919237613678.\n",
      "Epoch 159: Loss = 0.21777281165122986.\n",
      "Epoch 160: Loss = 0.21413198113441467.\n",
      "Epoch 161: Loss = 0.21055637300014496.\n",
      "Epoch 162: Loss = 0.20704537630081177.\n",
      "Epoch 163: Loss = 0.20359842479228973.\n",
      "Epoch 164: Loss = 0.20021483302116394.\n",
      "Epoch 165: Loss = 0.1968938112258911.\n",
      "Epoch 166: Loss = 0.1936347335577011.\n",
      "Epoch 167: Loss = 0.19043681025505066.\n",
      "Epoch 168: Loss = 0.18729901313781738.\n",
      "Epoch 169: Loss = 0.18422067165374756.\n",
      "Epoch 170: Loss = 0.18120084702968597.\n",
      "Epoch 171: Loss = 0.17823854088783264.\n",
      "Epoch 172: Loss = 0.1753329336643219.\n",
      "Epoch 173: Loss = 0.17248305678367615.\n",
      "Epoch 174: Loss = 0.16968786716461182.\n",
      "Epoch 175: Loss = 0.16694656014442444.\n",
      "Epoch 176: Loss = 0.16425800323486328.\n",
      "Epoch 177: Loss = 0.1616213619709015.\n",
      "Epoch 178: Loss = 0.15903569757938385.\n",
      "Epoch 179: Loss = 0.15649987757205963.\n",
      "Epoch 180: Loss = 0.15401309728622437.\n",
      "Epoch 181: Loss = 0.15157438814640045.\n",
      "Epoch 182: Loss = 0.14918281137943268.\n",
      "Epoch 183: Loss = 0.14683739840984344.\n",
      "Epoch 184: Loss = 0.1445373147726059.\n",
      "Epoch 185: Loss = 0.14228157699108124.\n",
      "Epoch 186: Loss = 0.14006933569908142.\n",
      "Epoch 187: Loss = 0.1378997564315796.\n",
      "Epoch 188: Loss = 0.13577185571193695.\n",
      "Epoch 189: Loss = 0.1336849480867386.\n",
      "Epoch 190: Loss = 0.1316380500793457.\n",
      "Epoch 191: Loss = 0.12963049113750458.\n",
      "Epoch 192: Loss = 0.1276613175868988.\n",
      "Epoch 193: Loss = 0.12572991847991943.\n",
      "Epoch 194: Loss = 0.12383536994457245.\n",
      "Epoch 195: Loss = 0.12197697162628174.\n",
      "Epoch 196: Loss = 0.1201540008187294.\n",
      "Epoch 197: Loss = 0.11836577951908112.\n",
      "Epoch 198: Loss = 0.11661145091056824.\n",
      "Epoch 199: Loss = 0.11489051580429077.\n",
      "Epoch 200: Loss = 0.11320208758115768.\n",
      "Epoch 201: Loss = 0.11154571175575256.\n",
      "Epoch 202: Loss = 0.10992057621479034.\n",
      "Epoch 203: Loss = 0.10832615941762924.\n",
      "Epoch 204: Loss = 0.10676175355911255.\n",
      "Epoch 205: Loss = 0.10522677004337311.\n",
      "Epoch 206: Loss = 0.10372063517570496.\n",
      "Epoch 207: Loss = 0.10224279761314392.\n",
      "Epoch 208: Loss = 0.10079259425401688.\n",
      "Epoch 209: Loss = 0.09936950355768204.\n",
      "Epoch 210: Loss = 0.0979730635881424.\n",
      "Epoch 211: Loss = 0.09660257399082184.\n",
      "Epoch 212: Loss = 0.09525763243436813.\n",
      "Epoch 213: Loss = 0.0939376950263977.\n",
      "Epoch 214: Loss = 0.09264221787452698.\n",
      "Epoch 215: Loss = 0.09137078374624252.\n",
      "Epoch 216: Loss = 0.09012292325496674.\n",
      "Epoch 217: Loss = 0.08889808505773544.\n",
      "Epoch 218: Loss = 0.08769580721855164.\n",
      "Epoch 219: Loss = 0.08651561290025711.\n",
      "Epoch 220: Loss = 0.08535721898078918.\n",
      "Epoch 221: Loss = 0.08421997725963593.\n",
      "Epoch 222: Loss = 0.08310365676879883.\n",
      "Epoch 223: Loss = 0.08200766891241074.\n",
      "Epoch 224: Loss = 0.08093167841434479.\n",
      "Epoch 225: Loss = 0.07987529039382935.\n",
      "Epoch 226: Loss = 0.07883819192647934.\n",
      "Epoch 227: Loss = 0.0778198093175888.\n",
      "Epoch 228: Loss = 0.07681997865438461.\n",
      "Epoch 229: Loss = 0.07583817839622498.\n",
      "Epoch 230: Loss = 0.07487402856349945.\n",
      "Epoch 231: Loss = 0.07392726838588715.\n",
      "Epoch 232: Loss = 0.0729975625872612.\n",
      "Epoch 233: Loss = 0.07208443433046341.\n",
      "Epoch 234: Loss = 0.07118768244981766.\n",
      "Epoch 235: Loss = 0.07030698657035828.\n",
      "Epoch 236: Loss = 0.06944189965724945.\n",
      "Epoch 237: Loss = 0.0685921460390091.\n",
      "Epoch 238: Loss = 0.06775754690170288.\n",
      "Epoch 239: Loss = 0.06693762540817261.\n",
      "Epoch 240: Loss = 0.06613217294216156.\n",
      "Epoch 241: Loss = 0.06534087657928467.\n",
      "Epoch 242: Loss = 0.06456350535154343.\n",
      "Epoch 243: Loss = 0.06379970163106918.\n",
      "Epoch 244: Loss = 0.06304929405450821.\n",
      "Epoch 245: Loss = 0.06231182441115379.\n",
      "Epoch 246: Loss = 0.061587218195199966.\n",
      "Epoch 247: Loss = 0.06087516248226166.\n",
      "Epoch 248: Loss = 0.06017535179853439.\n",
      "Epoch 249: Loss = 0.059487707912921906.\n",
      "Epoch 250: Loss = 0.05881171673536301.\n",
      "Epoch 251: Loss = 0.05814739689230919.\n",
      "Epoch 252: Loss = 0.05749434977769852.\n",
      "Epoch 253: Loss = 0.05685243010520935.\n",
      "Epoch 254: Loss = 0.05622135102748871.\n",
      "Epoch 255: Loss = 0.05560094863176346.\n",
      "Epoch 256: Loss = 0.05499099940061569.\n",
      "Epoch 257: Loss = 0.05439130216836929.\n",
      "Epoch 258: Loss = 0.05380161479115486.\n",
      "Epoch 259: Loss = 0.05322175845503807.\n",
      "Epoch 260: Loss = 0.052651580423116684.\n",
      "Epoch 261: Loss = 0.052090782672166824.\n",
      "Epoch 262: Loss = 0.05153927206993103.\n",
      "Epoch 263: Loss = 0.05099678039550781.\n",
      "Epoch 264: Loss = 0.05046330392360687.\n",
      "Epoch 265: Loss = 0.049938466399908066.\n",
      "Epoch 266: Loss = 0.04942212998867035.\n",
      "Epoch 267: Loss = 0.04891426861286163.\n",
      "Epoch 268: Loss = 0.04841453954577446.\n",
      "Epoch 269: Loss = 0.047922879457473755.\n",
      "Epoch 270: Loss = 0.047439079731702805.\n",
      "Epoch 271: Loss = 0.04696305841207504.\n",
      "Epoch 272: Loss = 0.046494562178850174.\n",
      "Epoch 273: Loss = 0.04603354260325432.\n",
      "Epoch 274: Loss = 0.045579783618450165.\n",
      "Epoch 275: Loss = 0.04513316974043846.\n",
      "Epoch 276: Loss = 0.04469352960586548.\n",
      "Epoch 277: Loss = 0.044260766357183456.\n",
      "Epoch 278: Loss = 0.043834786862134933.\n",
      "Epoch 279: Loss = 0.043415337800979614.\n",
      "Epoch 280: Loss = 0.04300237447023392.\n",
      "Epoch 281: Loss = 0.04259580746293068.\n",
      "Epoch 282: Loss = 0.04219542816281319.\n",
      "Epoch 283: Loss = 0.04180111363530159.\n",
      "Epoch 284: Loss = 0.04141278937458992.\n",
      "Epoch 285: Loss = 0.04103035107254982.\n",
      "Epoch 286: Loss = 0.040653716772794724.\n",
      "Epoch 287: Loss = 0.040282733738422394.\n",
      "Epoch 288: Loss = 0.03991726040840149.\n",
      "Epoch 289: Loss = 0.03955722600221634.\n",
      "Epoch 290: Loss = 0.039202455431222916.\n",
      "Epoch 291: Loss = 0.03885302692651749.\n",
      "Epoch 292: Loss = 0.03850870579481125.\n",
      "Epoch 293: Loss = 0.03816942498087883.\n",
      "Epoch 294: Loss = 0.0378350168466568.\n",
      "Epoch 295: Loss = 0.03750551491975784.\n",
      "Epoch 296: Loss = 0.03718075156211853.\n",
      "Epoch 297: Loss = 0.03686070442199707.\n",
      "Epoch 298: Loss = 0.03654518723487854.\n",
      "Epoch 299: Loss = 0.03623419627547264.\n",
      "Epoch 300: Loss = 0.035927630960941315.\n",
      "Testing:\n",
      "tensor([[-0.0301, -3.9982, -4.4787],\n",
      "        [-2.9359, -0.1329, -2.6395],\n",
      "        [-3.3665, -1.9442, -0.1955],\n",
      "        [-0.0492, -4.8472, -3.2148],\n",
      "        [-5.1040, -0.0334, -3.6186]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "WORD_EMBEDDING_DIM = 5\n",
    "CHAR_EMBEDDING_DIM = 6\n",
    "CHAR_REPR_DIM = 3\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Create and fill word-to-index, character-to-index and tag-to-index dictionaries\n",
    "word_to_ix = {}\n",
    "char_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "for seq, tags in training_data:\n",
    "    for word in seq:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "        for char in word:\n",
    "            if char not in char_to_ix:\n",
    "                char_to_ix[char] = len(char_to_ix)\n",
    "\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "\n",
    "def make_ixs(seq, to_ix):\n",
    "    ixs = torch.tensor([to_ix[w] for w in seq])\n",
    "    return ixs\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"LSTM part-os-speech (PoS) tagger.\"\"\"\n",
    "\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, char_repr_dim, hidden_dim, vocab_size, chars_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.char_repr_dim = char_repr_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        self.char_embeddings = nn.Embedding(chars_size, char_embedding_dim)\n",
    "\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_repr_dim)\n",
    "        self.word_lstm = nn.LSTM(word_embedding_dim + char_repr_dim, hidden_dim)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "        self.char_lstm_hidden = (torch.zeros(1, 1, self.char_repr_dim),\n",
    "                                 torch.zeros(1, 1, self.char_repr_dim))\n",
    "        self.word_lstm_hidden = (torch.zeros(1, 1, self.hidden_dim),\n",
    "                                 torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def init_word_hidden(self):\n",
    "        \"\"\"Initialise word LSTM hidden state.\"\"\"\n",
    "        self.word_lstm_hidden = (torch.zeros(1, 1, self.hidden_dim),\n",
    "                                 torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def init_char_hidden(self):\n",
    "        \"\"\"Initialise character LSTM hidden state.\"\"\"\n",
    "        self.char_lstm_hidden = (torch.zeros(1, 1, self.char_repr_dim),\n",
    "                                 torch.zeros(1, 1, self.char_repr_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        sentence_length = len(sentence)\n",
    "        word_characters_ixs = {}\n",
    "        for word in sentence:\n",
    "            word_ix = torch.tensor([word_to_ix[word]])\n",
    "            char_ixs = make_ixs(word, char_to_ix)\n",
    "            word_characters_ixs[word_ix] = char_ixs\n",
    "\n",
    "        word_embeds = []\n",
    "        for word_ix, char_ixs in word_characters_ixs.items():\n",
    "            word_embed = self.word_embeddings(word_ix)\n",
    "\n",
    "            self.init_char_hidden()\n",
    "            c = None  # Character-level representation.\n",
    "            # Character-level representation is the LSTM output of the last character.\n",
    "            for char_ix in char_ixs:\n",
    "                char_embed = self.char_embeddings(char_ix)\n",
    "                c, self.char_lstm_hidden = self.char_lstm(\n",
    "                    char_embed.view(1, 1, -1), self.char_lstm_hidden)\n",
    "            word_embeds.append(word_embed)\n",
    "            word_embeds.append(c.view(1, -1))\n",
    "        word_embeds = torch.cat(word_embeds, 1)\n",
    "\n",
    "        lstm_out, self.word_lstm_hidden = self.word_lstm(\n",
    "            # Each sentence embedding dimensions are word embedding dimensions + character representation dimensions\n",
    "            word_embeds.view(sentence_length, 1, -1),\n",
    "            self.word_lstm_hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(sentence_length, -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "model = LSTMTagger(WORD_EMBEDDING_DIM,\n",
    "                   CHAR_EMBEDDING_DIM, CHAR_REPR_DIM,\n",
    "                   HIDDEN_DIM,\n",
    "                   len(word_to_ix), len(char_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training\n",
    "for epoch in range(300):\n",
    "    total_loss = torch.tensor(0.)\n",
    "\n",
    "    for sentence, tags in training_data:\n",
    "        targets = make_ixs(tags, tag_to_ix)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        model.init_word_hidden()\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    print('Epoch {}: Loss = {}.'.format(epoch + 1, loss.item()))\n",
    "\n",
    "# Testing\n",
    "print('Testing:')\n",
    "with torch.no_grad():\n",
    "    inputs = training_data[0][0]\n",
    "\n",
    "    model.init_word_hidden()\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
